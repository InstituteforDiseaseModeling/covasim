{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# T7 - Calibration\n",
    "\n",
    "We saw in Tutorial 4 how to load and plot data. But the next step is to actually *calibrate* the model to the data, i.e. find the model parameters that are the most likely explanation for the observed data. This tutorial gives an introduction to the Fit object and some recipes for optimization approaches.\n",
    "\n",
    "## The Fit object\n",
    "\n",
    "The Fit object is responsible for quantifying how well a given model run matches the data. Let's consider a simple example, building on Tutorial 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covasim as cv\n",
    "cv.options.set(dpi=100, show=False, close=True, verbose=0) # Standard options for Jupyter notebook\n",
    "\n",
    "pars = dict(\n",
    "    start_day = '2020-02-01',\n",
    "    end_day   = '2020-04-11',\n",
    "    beta      = 0.015,\n",
    ")\n",
    "sim = cv.Sim(pars=pars, datafile='example_data.csv', interventions=cv.test_num(daily_tests='data'))\n",
    "sim.run()\n",
    "sim.plot(to_plot=['cum_tests', 'cum_diagnoses', 'cum_deaths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that tests match extremely well (they're input data!), diagnoses match reasonably well, and deaths match poorly. Can the Fit object capture our intuition about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sim.compute_fit()\n",
    "print(fit.mismatches)\n",
    "print(fit.mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the results seem to match our intuition. (Note that by default the Fit object uses normalized absolute difference, but other estimates, such as mean squared error, are also possible.)\n",
    "\n",
    "What if we improve the fit? Does the mismatch reduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.initialize(reset=True) # Reinitialize the sim\n",
    "sim['rel_death_prob'] = 2 # Double the death rate since deaths were too low\n",
    "\n",
    "# Rerun and compute fit\n",
    "sim.run()\n",
    "fit = sim.compute_fit()\n",
    "\n",
    "# Output\n",
    "sim.plot()\n",
    "fit.plot()\n",
    "print(fit.mismatches)\n",
    "print(fit.mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the fit is improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration approaches\n",
    "\n",
    "Calibration is a complex and dark art and cannot be covered fully here; many books have been written about it and it continues to be an area of active research. A good review article about calibrating agent-based models like Covasim is available [here](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007893). Calibration is usually expressed as an optimization problem: specifically, find a vector of parameters *θ* that minimizes the mismatch between the data *D* and the model *M(θ)*.\n",
    "\n",
    "In practice, most calibration is done simply by hand, as in the example above. Once deaths are \"calibrated\", the user might modify testing assumptions so that the diagnoses match. Since we are only fitting to deaths and diagnoses, the model is then \"calibrated\".\n",
    "\n",
    "However, automated approaches to calibration are possible as well. The simplest is probably the built-in SciPy optimization functions, e.g. `scipy.optimize`. A wrinkle here is that normal gradient descent methods **will not work** with Covasim or other agent-based models, due to the stochastic variability between model runs that makes the landscape very \"bumpy\". One way of getting around this is to use many different runs and take the average, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import covasim as cv\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def objective(x, n_runs=10):\n",
    "    print(f'Running sim for beta={x[0]}, rel_death_prob={x[1]}')\n",
    "    pars = dict(\n",
    "        start_day      = '2020-02-01',\n",
    "        end_day        = '2020-04-11',\n",
    "        beta           = x[0],\n",
    "        rel_death_prob = x[1],\n",
    "        verbose        = 0,\n",
    "    )\n",
    "    sim = cv.Sim(pars=pars, datafile='/home/cliffk/idm/covasim/docs/tutorials/example_data.csv', interventions=cv.test_num(daily_tests='data'))\n",
    "    msim = cv.MultiSim(sim)\n",
    "    msim.run(n_runs=n_runs)\n",
    "    mismatches = []\n",
    "    for sim in msim.sims:\n",
    "        fit = sim.compute_fit()\n",
    "        mismatches.append(fit.mismatch)\n",
    "    mismatch = np.mean(mismatches)\n",
    "    return mismatch\n",
    "\n",
    "guess = [0.015, 1] # Initial guess of parameters -- beta and relative death probability\n",
    "pars = scipy.optimize.minimize(objective, x0=guess, method='nelder-mead') # Run the optimization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should converge after roughly 3-10 minutes, although you will likely find that the improvement is minimal.\n",
    "\n",
    "What's happening here? Trying to overcome the limitations of an algorithm that expects deterministic results simply by running more sims is fairly futile – if you run *N* sims and average them together, you've only reduced noise by √*N*, i.e. you have to average together 100 sims to reduce noise by a factor of 10, and even that might not be enough. Clearly, we need a more powerful approach.\n",
    "\n",
    "One such package we have found works reasonably well is called [Optuna](https://optuna.org/). You are strongly encouraged to read its documentation, but below is a full example to help get you started. You may wish to copy this example into a separate .py file and run it outside of the Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example for running Optuna\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sciris as sc\n",
    "import covasim as cv\n",
    "import optuna as op\n",
    "\n",
    "# Create a (mutable) dictionary for global settings\n",
    "g = sc.objdict()\n",
    "g.name      = 'my-example-calibration'\n",
    "g.db_name   = f'{g.name}.db'\n",
    "g.storage   = f'sqlite:///{g.db_name}'\n",
    "g.n_workers = 4 # Define how many workers to run in parallel\n",
    "g.n_trials = 25 # Define the number of trials, i.e. sim runs, per worker\n",
    "\n",
    "\n",
    "def run_sim(pars, label=None, return_sim=False):\n",
    "    ''' Create and run a simulation '''\n",
    "    pars = dict(\n",
    "        pop_size       = 10_000,\n",
    "        start_day      = '2020-02-01',\n",
    "        end_day        = '2020-04-11',\n",
    "        beta           = pars[\"beta\"],\n",
    "        rel_death_prob = pars[\"rel_death_prob\"],\n",
    "        interventions  = cv.test_num(daily_tests='data'),\n",
    "        verbose        = 0,\n",
    "    )\n",
    "    sim = cv.Sim(pars=pars, datafile='example_data.csv', label=label)\n",
    "    sim.run()\n",
    "    fit = sim.compute_fit()\n",
    "    if return_sim:\n",
    "        return sim\n",
    "    else:\n",
    "        return fit.mismatch\n",
    "\n",
    "\n",
    "def run_trial(trial):\n",
    "    ''' Define the objective for Optuna '''\n",
    "    pars = {}\n",
    "    pars[\"beta\"]           = trial.suggest_uniform('beta', 0.005, 0.020) # Sample from beta values within this range\n",
    "    pars[\"rel_death_prob\"] = trial.suggest_uniform('rel_death_prob', 0.5, 3.0) # Sample from beta values within this range\n",
    "    mismatch = run_sim(pars)\n",
    "    return mismatch\n",
    "\n",
    "\n",
    "def worker():\n",
    "    ''' Run a single worker '''\n",
    "    study = op.load_study(storage=g.storage, study_name=g.name)\n",
    "    output = study.optimize(run_trial, n_trials=g.n_trials)\n",
    "    return output\n",
    "\n",
    "\n",
    "def run_workers():\n",
    "    ''' Run multiple workers in parallel '''\n",
    "    output = sc.parallelize(worker, g.n_workers)\n",
    "    return output\n",
    "\n",
    "\n",
    "def make_study():\n",
    "    ''' Make a study, deleting one if it already exists '''\n",
    "    if os.path.exists(g.db_name):\n",
    "        os.remove(g.db_name)\n",
    "        print(f'Removed existing calibration {g.db_name}')\n",
    "    output = op.create_study(storage=g.storage, study_name=g.name)\n",
    "    return output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Run the optimization\n",
    "    t0 = sc.tic()\n",
    "    make_study()\n",
    "    run_workers()\n",
    "    study = op.load_study(storage=g.storage, study_name=g.name)\n",
    "    best_pars = study.best_params\n",
    "    T = sc.toc(t0, output=True)\n",
    "    print(f'\\n\\nOutput: {best_pars}, time: {T:0.1f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "initial_pars = dict(beta=0.015, rel_death_prob=1.0)\n",
    "before = run_sim(pars=initial_pars, label='Before calibration', return_sim=True)\n",
    "after  = run_sim(pars=best_pars,    label='After calibration',  return_sim=True)\n",
    "msim = cv.MultiSim([before, after])\n",
    "msim.plot(to_plot=['cum_tests', 'cum_diagnoses', 'cum_deaths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to `scipy.optimize.minimize()`, Optuna took less time and produced a much better fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
